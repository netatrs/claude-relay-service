const axios = require('axios')
const ProxyHelper = require('../utils/proxyHelper')
const logger = require('../utils/logger')
const { filterForOpenAI } = require('../utils/headerFilter')
const openaiResponsesAccountService = require('./openaiResponsesAccountService')
const apiKeyService = require('./apiKeyService')
const unifiedOpenAIScheduler = require('./unifiedOpenAIScheduler')
const config = require('../../config/config')
const crypto = require('crypto')

// ÊäΩÂèñÁºìÂ≠òÂÜôÂÖ• tokenÔºåÂÖºÂÆπÂ§öÁßçÂ≠óÊÆµÂëΩÂêç
function extractCacheCreationTokens(usageData) {
  if (!usageData || typeof usageData !== 'object') {
    return 0
  }

  const details = usageData.input_tokens_details || usageData.prompt_tokens_details || {}
  const candidates = [
    details.cache_creation_input_tokens,
    details.cache_creation_tokens,
    usageData.cache_creation_input_tokens,
    usageData.cache_creation_tokens
  ]

  for (const value of candidates) {
    if (value !== undefined && value !== null && value !== '') {
      const parsed = Number(value)
      if (!Number.isNaN(parsed)) {
        return parsed
      }
    }
  }

  return 0
}

class OpenAIResponsesRelayService {
  constructor() {
    this.defaultTimeout = config.requestTimeout || 600000
  }

  // Â§ÑÁêÜËØ∑Ê±ÇËΩ¨Âèë
  async handleRequest(req, res, account, apiKeyData) {
    let abortController = null
    // Ëé∑Âèñ‰ºöËØùÂìàÂ∏åÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâ
    const sessionId = req.headers['session_id'] || req.body?.session_id
    const sessionHash = sessionId
      ? crypto.createHash('sha256').update(sessionId).digest('hex')
      : null

    try {
      // Ëé∑ÂèñÂÆåÊï¥ÁöÑË¥¶Êà∑‰ø°ÊÅØÔºàÂåÖÂê´Ëß£ÂØÜÁöÑ API KeyÔºâ
      const fullAccount = await openaiResponsesAccountService.getAccount(account.id)
      if (!fullAccount) {
        throw new Error('Account not found')
      }

      // ÂàõÂª∫ AbortController Áî®‰∫éÂèñÊ∂àËØ∑Ê±Ç
      abortController = new AbortController()

      // ËÆæÁΩÆÂÆ¢Êà∑Á´ØÊñ≠ÂºÄÁõëÂê¨Âô®
      const handleClientDisconnect = () => {
        logger.info('üîå Client disconnected, aborting OpenAI-Responses request')
        if (abortController && !abortController.signal.aborted) {
          abortController.abort()
        }
      }

      // ÁõëÂê¨ÂÆ¢Êà∑Á´ØÊñ≠ÂºÄ‰∫ã‰ª∂
      req.once('close', handleClientDisconnect)
      res.once('close', handleClientDisconnect)

      // ÊûÑÂª∫ÁõÆÊ†á URL
      const targetUrl = `${fullAccount.baseApi}${req.path}`
      logger.info(`üéØ Forwarding to: ${targetUrl}`)

      // ÊûÑÂª∫ËØ∑Ê±ÇÂ§¥ - ‰ΩøÁî®Áªü‰∏ÄÁöÑ headerFilter ÁßªÈô§ CDN headers
      const headers = {
        ...filterForOpenAI(req.headers),
        Authorization: `Bearer ${fullAccount.apiKey}`,
        'Content-Type': 'application/json'
      }

      // Â§ÑÁêÜ User-Agent
      if (fullAccount.userAgent) {
        // ‰ΩøÁî®Ëá™ÂÆö‰πâ User-Agent
        headers['User-Agent'] = fullAccount.userAgent
        logger.debug(`üì± Using custom User-Agent: ${fullAccount.userAgent}`)
      } else if (req.headers['user-agent']) {
        // ÈÄè‰º†ÂéüÂßã User-Agent
        headers['User-Agent'] = req.headers['user-agent']
        logger.debug(`üì± Forwarding original User-Agent: ${req.headers['user-agent']}`)
      }

      // ÈÖçÁΩÆËØ∑Ê±ÇÈÄâÈ°π
      const requestOptions = {
        method: req.method,
        url: targetUrl,
        headers,
        data: req.body,
        timeout: this.defaultTimeout,
        responseType: req.body?.stream ? 'stream' : 'json',
        validateStatus: () => true, // ÂÖÅËÆ∏Â§ÑÁêÜÊâÄÊúâÁä∂ÊÄÅÁ†Å
        signal: abortController.signal
      }

      // ÈÖçÁΩÆ‰ª£ÁêÜÔºàÂ¶ÇÊûúÊúâÔºâ
      if (fullAccount.proxy) {
        const proxyAgent = ProxyHelper.createProxyAgent(fullAccount.proxy)
        if (proxyAgent) {
          requestOptions.httpAgent = proxyAgent
          requestOptions.httpsAgent = proxyAgent
          requestOptions.proxy = false
          logger.info(
            `üåê Using proxy for OpenAI-Responses: ${ProxyHelper.getProxyDescription(fullAccount.proxy)}`
          )
        }
      }

      // ËÆ∞ÂΩïËØ∑Ê±Ç‰ø°ÊÅØ
      logger.info('üì§ OpenAI-Responses relay request', {
        accountId: account.id,
        accountName: account.name,
        targetUrl,
        method: req.method,
        stream: req.body?.stream || false,
        model: req.body?.model || 'unknown',
        userAgent: headers['User-Agent'] || 'not set'
      })

      // ÂèëÈÄÅËØ∑Ê±Ç
      const response = await axios(requestOptions)

      // Â§ÑÁêÜ 429 ÈôêÊµÅÈîôËØØ
      if (response.status === 429) {
        const { resetsInSeconds, errorData } = await this._handle429Error(
          account,
          response,
          req.body?.stream,
          sessionHash
        )

        // ËøîÂõûÈîôËØØÂìçÂ∫îÔºà‰ΩøÁî®Â§ÑÁêÜÂêéÁöÑÊï∞ÊçÆÔºåÈÅøÂÖçÂæ™ÁéØÂºïÁî®Ôºâ
        const errorResponse = errorData || {
          error: {
            message: 'Rate limit exceeded',
            type: 'rate_limit_error',
            code: 'rate_limit_exceeded',
            resets_in_seconds: resetsInSeconds
          }
        }
        return res.status(429).json(errorResponse)
      }

      // Â§ÑÁêÜÂÖ∂‰ªñÈîôËØØÁä∂ÊÄÅÁ†Å
      if (response.status >= 400) {
        // Â§ÑÁêÜÊµÅÂºèÈîôËØØÂìçÂ∫î
        let errorData = response.data
        if (response.data && typeof response.data.pipe === 'function') {
          // ÊµÅÂºèÂìçÂ∫îÈúÄË¶ÅÂÖàËØªÂèñÂÜÖÂÆπ
          const chunks = []
          await new Promise((resolve) => {
            response.data.on('data', (chunk) => chunks.push(chunk))
            response.data.on('end', resolve)
            response.data.on('error', resolve)
            setTimeout(resolve, 5000) // Ë∂ÖÊó∂‰øùÊä§
          })
          const fullResponse = Buffer.concat(chunks).toString()

          // Â∞ùËØïËß£ÊûêÈîôËØØÂìçÂ∫î
          try {
            if (fullResponse.includes('data: ')) {
              // SSEÊ†ºÂºè
              const lines = fullResponse.split('\n')
              for (const line of lines) {
                if (line.startsWith('data: ')) {
                  const jsonStr = line.slice(6).trim()
                  if (jsonStr && jsonStr !== '[DONE]') {
                    errorData = JSON.parse(jsonStr)
                    break
                  }
                }
              }
            } else {
              // ÊôÆÈÄöJSON
              errorData = JSON.parse(fullResponse)
            }
          } catch (e) {
            logger.error('Failed to parse error response:', e)
            errorData = { error: { message: fullResponse || 'Unknown error' } }
          }
        }

        logger.error('OpenAI-Responses API error', {
          status: response.status,
          statusText: response.statusText,
          errorData
        })

        if (response.status === 401) {
          let reason = 'OpenAI ResponsesË¥¶Âè∑ËÆ§ËØÅÂ§±Ë¥•Ôºà401ÈîôËØØÔºâ'
          if (errorData) {
            if (typeof errorData === 'string' && errorData.trim()) {
              reason = `OpenAI ResponsesË¥¶Âè∑ËÆ§ËØÅÂ§±Ë¥•Ôºà401ÈîôËØØÔºâÔºö${errorData.trim()}`
            } else if (
              errorData.error &&
              typeof errorData.error.message === 'string' &&
              errorData.error.message.trim()
            ) {
              reason = `OpenAI ResponsesË¥¶Âè∑ËÆ§ËØÅÂ§±Ë¥•Ôºà401ÈîôËØØÔºâÔºö${errorData.error.message.trim()}`
            } else if (typeof errorData.message === 'string' && errorData.message.trim()) {
              reason = `OpenAI ResponsesË¥¶Âè∑ËÆ§ËØÅÂ§±Ë¥•Ôºà401ÈîôËØØÔºâÔºö${errorData.message.trim()}`
            }
          }

          try {
            await unifiedOpenAIScheduler.markAccountUnauthorized(
              account.id,
              'openai-responses',
              sessionHash,
              reason
            )
          } catch (markError) {
            logger.error(
              '‚ùå Failed to mark OpenAI-Responses account unauthorized after 401:',
              markError
            )
          }

          let unauthorizedResponse = errorData
          if (
            !unauthorizedResponse ||
            typeof unauthorizedResponse !== 'object' ||
            unauthorizedResponse.pipe ||
            Buffer.isBuffer(unauthorizedResponse)
          ) {
            const fallbackMessage =
              typeof errorData === 'string' && errorData.trim() ? errorData.trim() : 'Unauthorized'
            unauthorizedResponse = {
              error: {
                message: fallbackMessage,
                type: 'unauthorized',
                code: 'unauthorized'
              }
            }
          }

          // Ê∏ÖÁêÜÁõëÂê¨Âô®
          req.removeListener('close', handleClientDisconnect)
          res.removeListener('close', handleClientDisconnect)

          return res.status(401).json(unauthorizedResponse)
        }

        // Ê∏ÖÁêÜÁõëÂê¨Âô®
        req.removeListener('close', handleClientDisconnect)
        res.removeListener('close', handleClientDisconnect)

        return res.status(response.status).json(errorData)
      }

      // Êõ¥Êñ∞ÊúÄÂêé‰ΩøÁî®Êó∂Èó¥
      await openaiResponsesAccountService.updateAccount(account.id, {
        lastUsedAt: new Date().toISOString()
      })

      // Â§ÑÁêÜÊµÅÂºèÂìçÂ∫î
      if (req.body?.stream && response.data && typeof response.data.pipe === 'function') {
        return this._handleStreamResponse(
          response,
          res,
          account,
          apiKeyData,
          req.body?.model,
          handleClientDisconnect,
          req
        )
      }

      // Â§ÑÁêÜÈùûÊµÅÂºèÂìçÂ∫î
      return this._handleNormalResponse(response, res, account, apiKeyData, req.body?.model)
    } catch (error) {
      // Ê∏ÖÁêÜ AbortController
      if (abortController && !abortController.signal.aborted) {
        abortController.abort()
      }

      // ÂÆâÂÖ®Âú∞ËÆ∞ÂΩïÈîôËØØÔºåÈÅøÂÖçÂæ™ÁéØÂºïÁî®
      const errorInfo = {
        message: error.message,
        code: error.code,
        status: error.response?.status,
        statusText: error.response?.statusText
      }
      logger.error('OpenAI-Responses relay error:', errorInfo)

      // Ê£ÄÊü•ÊòØÂê¶ÊòØÁΩëÁªúÈîôËØØ
      if (error.code === 'ECONNREFUSED' || error.code === 'ETIMEDOUT') {
        await openaiResponsesAccountService.updateAccount(account.id, {
          status: 'error',
          errorMessage: `Connection error: ${error.code}`
        })
      }

      // Â¶ÇÊûúÂ∑≤ÁªèÂèëÈÄÅ‰∫ÜÂìçÂ∫îÂ§¥ÔºåÁõ¥Êé•ÁªìÊùü
      if (res.headersSent) {
        return res.end()
      }

      // Ê£ÄÊü•ÊòØÂê¶ÊòØaxiosÈîôËØØÂπ∂ÂåÖÂê´ÂìçÂ∫î
      if (error.response) {
        // Â§ÑÁêÜaxiosÈîôËØØÂìçÂ∫î
        const status = error.response.status || 500
        let errorData = {
          error: {
            message: error.response.statusText || 'Request failed',
            type: 'api_error',
            code: error.code || 'unknown'
          }
        }

        // Â¶ÇÊûúÂìçÂ∫îÂåÖÂê´Êï∞ÊçÆÔºåÂ∞ùËØï‰ΩøÁî®ÂÆÉ
        if (error.response.data) {
          // Ê£ÄÊü•ÊòØÂê¶ÊòØÊµÅ
          if (typeof error.response.data === 'object' && !error.response.data.pipe) {
            errorData = error.response.data
          } else if (typeof error.response.data === 'string') {
            try {
              errorData = JSON.parse(error.response.data)
            } catch (e) {
              errorData.error.message = error.response.data
            }
          }
        }

        if (status === 401) {
          let reason = 'OpenAI ResponsesË¥¶Âè∑ËÆ§ËØÅÂ§±Ë¥•Ôºà401ÈîôËØØÔºâ'
          if (errorData) {
            if (typeof errorData === 'string' && errorData.trim()) {
              reason = `OpenAI ResponsesË¥¶Âè∑ËÆ§ËØÅÂ§±Ë¥•Ôºà401ÈîôËØØÔºâÔºö${errorData.trim()}`
            } else if (
              errorData.error &&
              typeof errorData.error.message === 'string' &&
              errorData.error.message.trim()
            ) {
              reason = `OpenAI ResponsesË¥¶Âè∑ËÆ§ËØÅÂ§±Ë¥•Ôºà401ÈîôËØØÔºâÔºö${errorData.error.message.trim()}`
            } else if (typeof errorData.message === 'string' && errorData.message.trim()) {
              reason = `OpenAI ResponsesË¥¶Âè∑ËÆ§ËØÅÂ§±Ë¥•Ôºà401ÈîôËØØÔºâÔºö${errorData.message.trim()}`
            }
          }

          try {
            await unifiedOpenAIScheduler.markAccountUnauthorized(
              account.id,
              'openai-responses',
              sessionHash,
              reason
            )
          } catch (markError) {
            logger.error(
              '‚ùå Failed to mark OpenAI-Responses account unauthorized in catch handler:',
              markError
            )
          }

          let unauthorizedResponse = errorData
          if (
            !unauthorizedResponse ||
            typeof unauthorizedResponse !== 'object' ||
            unauthorizedResponse.pipe ||
            Buffer.isBuffer(unauthorizedResponse)
          ) {
            const fallbackMessage =
              typeof errorData === 'string' && errorData.trim() ? errorData.trim() : 'Unauthorized'
            unauthorizedResponse = {
              error: {
                message: fallbackMessage,
                type: 'unauthorized',
                code: 'unauthorized'
              }
            }
          }

          return res.status(401).json(unauthorizedResponse)
        }

        return res.status(status).json(errorData)
      }

      // ÂÖ∂‰ªñÈîôËØØ
      return res.status(500).json({
        error: {
          message: 'Internal server error',
          type: 'internal_error',
          details: error.message
        }
      })
    }
  }

  // Â§ÑÁêÜÊµÅÂºèÂìçÂ∫î
  async _handleStreamResponse(
    response,
    res,
    account,
    apiKeyData,
    requestedModel,
    handleClientDisconnect,
    req
  ) {
    // ËÆæÁΩÆ SSE ÂìçÂ∫îÂ§¥
    res.setHeader('Content-Type', 'text/event-stream')
    res.setHeader('Cache-Control', 'no-cache')
    res.setHeader('Connection', 'keep-alive')
    res.setHeader('X-Accel-Buffering', 'no')

    let usageData = null
    let actualModel = null
    let buffer = ''
    let rateLimitDetected = false
    let rateLimitResetsInSeconds = null
    let streamEnded = false

    // Ëß£Êûê SSE ‰∫ã‰ª∂‰ª•ÊçïËé∑ usage Êï∞ÊçÆÂíå model
    const parseSSEForUsage = (data) => {
      const lines = data.split('\n')

      for (const line of lines) {
        if (line.startsWith('data:')) {
          try {
            const jsonStr = line.slice(5).trim()
            if (jsonStr === '[DONE]') {
              continue
            }

            const eventData = JSON.parse(jsonStr)

            // Ê£ÄÊü•ÊòØÂê¶ÊòØ response.completed ‰∫ã‰ª∂ÔºàOpenAI-Responses Ê†ºÂºèÔºâ
            if (eventData.type === 'response.completed' && eventData.response) {
              // ‰ªéÂìçÂ∫î‰∏≠Ëé∑ÂèñÁúüÂÆûÁöÑ model
              if (eventData.response.model) {
                actualModel = eventData.response.model
                logger.debug(`üìä Captured actual model from response.completed: ${actualModel}`)
              }

              // Ëé∑Âèñ usage Êï∞ÊçÆ - OpenAI-Responses Ê†ºÂºèÂú® response.usage ‰∏ã
              if (eventData.response.usage) {
                usageData = eventData.response.usage
                logger.info('üìä Successfully captured usage data from OpenAI-Responses:', {
                  input_tokens: usageData.input_tokens,
                  output_tokens: usageData.output_tokens,
                  total_tokens: usageData.total_tokens
                })
              }
            }

            // Ê£ÄÊü•ÊòØÂê¶ÊúâÈôêÊµÅÈîôËØØ
            if (eventData.error) {
              // Ê£ÄÊü•Â§öÁßçÂèØËÉΩÁöÑÈôêÊµÅÈîôËØØÁ±ªÂûã
              if (
                eventData.error.type === 'rate_limit_error' ||
                eventData.error.type === 'usage_limit_reached' ||
                eventData.error.type === 'rate_limit_exceeded'
              ) {
                rateLimitDetected = true
                if (eventData.error.resets_in_seconds) {
                  rateLimitResetsInSeconds = eventData.error.resets_in_seconds
                  logger.warn(
                    `üö´ Rate limit detected in stream, resets in ${rateLimitResetsInSeconds} seconds (${Math.ceil(rateLimitResetsInSeconds / 60)} minutes)`
                  )
                }
              }
            }
          } catch (e) {
            // ÂøΩÁï•Ëß£ÊûêÈîôËØØ
          }
        }
      }
    }

    // ÁõëÂê¨Êï∞ÊçÆÊµÅ
    response.data.on('data', (chunk) => {
      try {
        const chunkStr = chunk.toString()

        // ËΩ¨ÂèëÊï∞ÊçÆÁªôÂÆ¢Êà∑Á´Ø
        if (!res.destroyed && !streamEnded) {
          res.write(chunk)
        }

        // ÂêåÊó∂Ëß£ÊûêÊï∞ÊçÆ‰ª•ÊçïËé∑ usage ‰ø°ÊÅØ
        buffer += chunkStr

        // Â§ÑÁêÜÂÆåÊï¥ÁöÑ SSE ‰∫ã‰ª∂
        if (buffer.includes('\n\n')) {
          const events = buffer.split('\n\n')
          buffer = events.pop() || ''

          for (const event of events) {
            if (event.trim()) {
              parseSSEForUsage(event)
            }
          }
        }
      } catch (error) {
        logger.error('Error processing stream chunk:', error)
      }
    })

    response.data.on('end', async () => {
      streamEnded = true

      // Â§ÑÁêÜÂâ©‰ΩôÁöÑ buffer
      if (buffer.trim()) {
        parseSSEForUsage(buffer)
      }

      // ËÆ∞ÂΩï‰ΩøÁî®ÁªüËÆ°
      if (usageData) {
        try {
          // OpenAI-Responses ‰ΩøÁî® input_tokens/output_tokensÔºåÊ†áÂáÜ OpenAI ‰ΩøÁî® prompt_tokens/completion_tokens
          const totalInputTokens = usageData.input_tokens || usageData.prompt_tokens || 0
          const outputTokens = usageData.output_tokens || usageData.completion_tokens || 0

          // ÊèêÂèñÁºìÂ≠òÁõ∏ÂÖ≥ÁöÑ tokensÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ
          const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0
          const cacheCreateTokens = extractCacheCreationTokens(usageData)
          // ËÆ°ÁÆóÂÆûÈôÖËæìÂÖ•tokenÔºàÊÄªËæìÂÖ•ÂáèÂéªÁºìÂ≠òÈÉ®ÂàÜÔºâ
          const actualInputTokens = Math.max(0, totalInputTokens - cacheReadTokens)

          const totalTokens =
            usageData.total_tokens || totalInputTokens + outputTokens + cacheCreateTokens
          const modelToRecord = actualModel || requestedModel || 'gpt-4'

          await apiKeyService.recordUsage(
            apiKeyData.id,
            actualInputTokens, // ‰º†ÈÄíÂÆûÈôÖËæìÂÖ•Ôºà‰∏çÂê´ÁºìÂ≠òÔºâ
            outputTokens,
            cacheCreateTokens,
            cacheReadTokens,
            modelToRecord,
            account.id
          )

          logger.info(
            `üìä Recorded usage - Input: ${totalInputTokens}(actual:${actualInputTokens}+cached:${cacheReadTokens}), CacheCreate: ${cacheCreateTokens}, Output: ${outputTokens}, Total: ${totalTokens}, Model: ${modelToRecord}`
          )

          // Êõ¥Êñ∞Ë¥¶Êà∑ÁöÑ token ‰ΩøÁî®ÁªüËÆ°
          await openaiResponsesAccountService.updateAccountUsage(account.id, totalTokens)

          // Êõ¥Êñ∞Ë¥¶Êà∑‰ΩøÁî®È¢ùÂ∫¶ÔºàÂ¶ÇÊûúËÆæÁΩÆ‰∫ÜÈ¢ùÂ∫¶ÈôêÂà∂Ôºâ
          if (parseFloat(account.dailyQuota) > 0) {
            // ‰ΩøÁî®CostCalculatorÊ≠£Á°ÆËÆ°ÁÆóË¥πÁî®ÔºàËÄÉËôëÁºìÂ≠òtokenÁöÑ‰∏çÂêå‰ª∑Ê†ºÔºâ
            const CostCalculator = require('../utils/costCalculator')
            const costInfo = CostCalculator.calculateCost(
              {
                input_tokens: actualInputTokens, // ÂÆûÈôÖËæìÂÖ•Ôºà‰∏çÂê´ÁºìÂ≠òÔºâ
                output_tokens: outputTokens,
                cache_creation_input_tokens: cacheCreateTokens,
                cache_read_input_tokens: cacheReadTokens
              },
              modelToRecord
            )
            await openaiResponsesAccountService.updateUsageQuota(account.id, costInfo.costs.total)
          }
        } catch (error) {
          logger.error('Failed to record usage:', error)
        }
      }

      // Â¶ÇÊûúÂú®ÊµÅÂºèÂìçÂ∫î‰∏≠Ê£ÄÊµãÂà∞ÈôêÊµÅ
      if (rateLimitDetected) {
        // ‰ΩøÁî®Áªü‰∏ÄË∞ÉÂ∫¶Âô®Â§ÑÁêÜÈôêÊµÅÔºà‰∏éÈùûÊµÅÂºèÂìçÂ∫î‰øùÊåÅ‰∏ÄËá¥Ôºâ
        const sessionId = req.headers['session_id'] || req.body?.session_id
        const sessionHash = sessionId
          ? crypto.createHash('sha256').update(sessionId).digest('hex')
          : null

        await unifiedOpenAIScheduler.markAccountRateLimited(
          account.id,
          'openai-responses',
          sessionHash,
          rateLimitResetsInSeconds
        )

        logger.warn(
          `üö´ Processing rate limit for OpenAI-Responses account ${account.id} from stream`
        )
      }

      // Ê∏ÖÁêÜÁõëÂê¨Âô®
      req.removeListener('close', handleClientDisconnect)
      res.removeListener('close', handleClientDisconnect)

      if (!res.destroyed) {
        res.end()
      }

      logger.info('Stream response completed', {
        accountId: account.id,
        hasUsage: !!usageData,
        actualModel: actualModel || 'unknown'
      })
    })

    response.data.on('error', (error) => {
      streamEnded = true
      logger.error('Stream error:', error)

      // Ê∏ÖÁêÜÁõëÂê¨Âô®
      req.removeListener('close', handleClientDisconnect)
      res.removeListener('close', handleClientDisconnect)

      if (!res.headersSent) {
        res.status(502).json({ error: { message: 'Upstream stream error' } })
      } else if (!res.destroyed) {
        res.end()
      }
    })

    // Â§ÑÁêÜÂÆ¢Êà∑Á´ØÊñ≠ÂºÄËøûÊé•
    const cleanup = () => {
      streamEnded = true
      try {
        response.data?.unpipe?.(res)
        response.data?.destroy?.()
      } catch (_) {
        // ÂøΩÁï•Ê∏ÖÁêÜÈîôËØØ
      }
    }

    req.on('close', cleanup)
    req.on('aborted', cleanup)
  }

  // Â§ÑÁêÜÈùûÊµÅÂºèÂìçÂ∫î
  async _handleNormalResponse(response, res, account, apiKeyData, requestedModel) {
    const responseData = response.data

    // ÊèêÂèñ usage Êï∞ÊçÆÂíåÂÆûÈôÖ model
    // ÊîØÊåÅ‰∏§ÁßçÊ†ºÂºèÔºöÁõ¥Êé•ÁöÑ usage ÊàñÂµåÂ•óÂú® response ‰∏≠ÁöÑ usage
    const usageData = responseData?.usage || responseData?.response?.usage
    const actualModel =
      responseData?.model || responseData?.response?.model || requestedModel || 'gpt-4'

    // ËÆ∞ÂΩï‰ΩøÁî®ÁªüËÆ°
    if (usageData) {
      try {
        // OpenAI-Responses ‰ΩøÁî® input_tokens/output_tokensÔºåÊ†áÂáÜ OpenAI ‰ΩøÁî® prompt_tokens/completion_tokens
        const totalInputTokens = usageData.input_tokens || usageData.prompt_tokens || 0
        const outputTokens = usageData.output_tokens || usageData.completion_tokens || 0

        // ÊèêÂèñÁºìÂ≠òÁõ∏ÂÖ≥ÁöÑ tokensÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ
        const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0
        const cacheCreateTokens = extractCacheCreationTokens(usageData)
        // ËÆ°ÁÆóÂÆûÈôÖËæìÂÖ•tokenÔºàÊÄªËæìÂÖ•ÂáèÂéªÁºìÂ≠òÈÉ®ÂàÜÔºâ
        const actualInputTokens = Math.max(0, totalInputTokens - cacheReadTokens)

        const totalTokens =
          usageData.total_tokens || totalInputTokens + outputTokens + cacheCreateTokens

        await apiKeyService.recordUsage(
          apiKeyData.id,
          actualInputTokens, // ‰º†ÈÄíÂÆûÈôÖËæìÂÖ•Ôºà‰∏çÂê´ÁºìÂ≠òÔºâ
          outputTokens,
          cacheCreateTokens,
          cacheReadTokens,
          actualModel,
          account.id
        )

        logger.info(
          `üìä Recorded non-stream usage - Input: ${totalInputTokens}(actual:${actualInputTokens}+cached:${cacheReadTokens}), CacheCreate: ${cacheCreateTokens}, Output: ${outputTokens}, Total: ${totalTokens}, Model: ${actualModel}`
        )

        // Êõ¥Êñ∞Ë¥¶Êà∑ÁöÑ token ‰ΩøÁî®ÁªüËÆ°
        await openaiResponsesAccountService.updateAccountUsage(account.id, totalTokens)

        // Êõ¥Êñ∞Ë¥¶Êà∑‰ΩøÁî®È¢ùÂ∫¶ÔºàÂ¶ÇÊûúËÆæÁΩÆ‰∫ÜÈ¢ùÂ∫¶ÈôêÂà∂Ôºâ
        if (parseFloat(account.dailyQuota) > 0) {
          // ‰ΩøÁî®CostCalculatorÊ≠£Á°ÆËÆ°ÁÆóË¥πÁî®ÔºàËÄÉËôëÁºìÂ≠òtokenÁöÑ‰∏çÂêå‰ª∑Ê†ºÔºâ
          const CostCalculator = require('../utils/costCalculator')
          const costInfo = CostCalculator.calculateCost(
            {
              input_tokens: actualInputTokens, // ÂÆûÈôÖËæìÂÖ•Ôºà‰∏çÂê´ÁºìÂ≠òÔºâ
              output_tokens: outputTokens,
              cache_creation_input_tokens: cacheCreateTokens,
              cache_read_input_tokens: cacheReadTokens
            },
            actualModel
          )
          await openaiResponsesAccountService.updateUsageQuota(account.id, costInfo.costs.total)
        }
      } catch (error) {
        logger.error('Failed to record usage:', error)
      }
    }

    // ËøîÂõûÂìçÂ∫î
    res.status(response.status).json(responseData)

    logger.info('Normal response completed', {
      accountId: account.id,
      status: response.status,
      hasUsage: !!usageData,
      model: actualModel
    })
  }

  // Â§ÑÁêÜ 429 ÈôêÊµÅÈîôËØØ
  async _handle429Error(account, response, isStream = false, sessionHash = null) {
    let resetsInSeconds = null
    let errorData = null

    try {
      // ÂØπ‰∫é429ÈîôËØØÔºåÂìçÂ∫îÂèØËÉΩÊòØJSONÊàñSSEÊ†ºÂºè
      if (isStream && response.data && typeof response.data.pipe === 'function') {
        // ÊµÅÂºèÂìçÂ∫îÈúÄË¶ÅÂÖàÊî∂ÈõÜÊï∞ÊçÆ
        const chunks = []
        await new Promise((resolve, reject) => {
          response.data.on('data', (chunk) => chunks.push(chunk))
          response.data.on('end', resolve)
          response.data.on('error', reject)
          // ËÆæÁΩÆË∂ÖÊó∂Èò≤Ê≠¢Êó†ÈôêÁ≠âÂæÖ
          setTimeout(resolve, 5000)
        })

        const fullResponse = Buffer.concat(chunks).toString()

        // Â∞ùËØïËß£ÊûêSSEÊ†ºÂºèÁöÑÈîôËØØÂìçÂ∫î
        if (fullResponse.includes('data: ')) {
          const lines = fullResponse.split('\n')
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              try {
                const jsonStr = line.slice(6).trim()
                if (jsonStr && jsonStr !== '[DONE]') {
                  errorData = JSON.parse(jsonStr)
                  break
                }
              } catch (e) {
                // ÁªßÁª≠Â∞ùËØï‰∏ã‰∏ÄË°å
              }
            }
          }
        }

        // Â¶ÇÊûúSSEËß£ÊûêÂ§±Ë¥•ÔºåÂ∞ùËØïÁõ¥Êé•Ëß£Êûê‰∏∫JSON
        if (!errorData) {
          try {
            errorData = JSON.parse(fullResponse)
          } catch (e) {
            logger.error('Failed to parse 429 error response:', e)
            logger.debug('Raw response:', fullResponse)
          }
        }
      } else if (response.data && typeof response.data !== 'object') {
        // Â¶ÇÊûúresponse.dataÊòØÂ≠óÁ¨¶‰∏≤ÔºåÂ∞ùËØïËß£Êûê‰∏∫JSON
        try {
          errorData = JSON.parse(response.data)
        } catch (e) {
          logger.error('Failed to parse 429 error response as JSON:', e)
          errorData = { error: { message: response.data } }
        }
      } else if (response.data && typeof response.data === 'object' && !response.data.pipe) {
        // ÈùûÊµÅÂºèÂìçÂ∫îÔºå‰∏îÊòØÂØπË±°ÔºåÁõ¥Êé•‰ΩøÁî®
        errorData = response.data
      }

      // ‰ªéÂìçÂ∫î‰Ωì‰∏≠ÊèêÂèñÈáçÁΩÆÊó∂Èó¥ÔºàOpenAI Ê†áÂáÜÊ†ºÂºèÔºâ
      if (errorData && errorData.error) {
        if (errorData.error.resets_in_seconds) {
          resetsInSeconds = errorData.error.resets_in_seconds
          logger.info(
            `üïê Rate limit will reset in ${resetsInSeconds} seconds (${Math.ceil(resetsInSeconds / 60)} minutes / ${Math.ceil(resetsInSeconds / 3600)} hours)`
          )
        } else if (errorData.error.resets_in) {
          // Êüê‰∫õ API ÂèØËÉΩ‰ΩøÁî®‰∏çÂêåÁöÑÂ≠óÊÆµÂêç
          resetsInSeconds = parseInt(errorData.error.resets_in)
          logger.info(
            `üïê Rate limit will reset in ${resetsInSeconds} seconds (${Math.ceil(resetsInSeconds / 60)} minutes / ${Math.ceil(resetsInSeconds / 3600)} hours)`
          )
        }
      }

      if (!resetsInSeconds) {
        logger.warn('‚ö†Ô∏è Could not extract reset time from 429 response, using default 60 minutes')
      }
    } catch (e) {
      logger.error('‚ö†Ô∏è Failed to parse rate limit error:', e)
    }

    // ‰ΩøÁî®Áªü‰∏ÄË∞ÉÂ∫¶Âô®Ê†áËÆ∞Ë¥¶Êà∑‰∏∫ÈôêÊµÅÁä∂ÊÄÅÔºà‰∏éÊôÆÈÄöOpenAIË¥¶Âè∑‰øùÊåÅ‰∏ÄËá¥Ôºâ
    await unifiedOpenAIScheduler.markAccountRateLimited(
      account.id,
      'openai-responses',
      sessionHash,
      resetsInSeconds
    )

    logger.warn('OpenAI-Responses account rate limited', {
      accountId: account.id,
      accountName: account.name,
      resetsInSeconds: resetsInSeconds || 'unknown',
      resetInMinutes: resetsInSeconds ? Math.ceil(resetsInSeconds / 60) : 60,
      resetInHours: resetsInSeconds ? Math.ceil(resetsInSeconds / 3600) : 1
    })

    // ËøîÂõûÂ§ÑÁêÜÂêéÁöÑÊï∞ÊçÆÔºåÈÅøÂÖçÂæ™ÁéØÂºïÁî®
    return { resetsInSeconds, errorData }
  }

  // ËøáÊª§ËØ∑Ê±ÇÂ§¥ - Â∑≤ËøÅÁßªÂà∞ headerFilter Â∑•ÂÖ∑Á±ª
  // Ê≠§ÊñπÊ≥ï‰øùÁïôÁî®‰∫éÂêëÂêéÂÖºÂÆπÔºåÂÆûÈôÖ‰ΩøÁî® filterForOpenAI()
  _filterRequestHeaders(headers) {
    return filterForOpenAI(headers)
  }

  // ‰º∞ÁÆóË¥πÁî®ÔºàÁÆÄÂåñÁâàÊú¨ÔºåÂÆûÈôÖÂ∫îËØ•Ê†πÊçÆ‰∏çÂêåÁöÑÂÆö‰ª∑Ê®°ÂûãÔºâ
  _estimateCost(model, inputTokens, outputTokens) {
    // ËøôÊòØ‰∏Ä‰∏™ÁÆÄÂåñÁöÑË¥πÁî®‰º∞ÁÆóÔºåÂÆûÈôÖÂ∫îËØ•Ê†πÊçÆ‰∏çÂêåÁöÑ API Êèê‰æõÂïÜÂíåÊ®°ÂûãÂÆö‰ª∑
    const rates = {
      'gpt-4': { input: 0.03, output: 0.06 }, // per 1K tokens
      'gpt-4-turbo': { input: 0.01, output: 0.03 },
      'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 },
      'claude-3-opus': { input: 0.015, output: 0.075 },
      'claude-3-sonnet': { input: 0.003, output: 0.015 },
      'claude-3-haiku': { input: 0.00025, output: 0.00125 }
    }

    // Êü•ÊâæÂåπÈÖçÁöÑÊ®°ÂûãÂÆö‰ª∑
    let rate = rates['gpt-3.5-turbo'] // ÈªòËÆ§‰ΩøÁî® GPT-3.5 ÁöÑ‰ª∑Ê†º
    for (const [modelKey, modelRate] of Object.entries(rates)) {
      if (model.toLowerCase().includes(modelKey.toLowerCase())) {
        rate = modelRate
        break
      }
    }

    const inputCost = (inputTokens / 1000) * rate.input
    const outputCost = (outputTokens / 1000) * rate.output
    return inputCost + outputCost
  }

  async testAccountConnection(accountId, responseStream) {
    const sendSSE = (type, data = {}) => {
      if (!responseStream.destroyed && !responseStream.writableEnded) {
        try {
          responseStream.write(`data: ${JSON.stringify({ type, ...data })}\n\n`)
        } catch { /* ignore */ }
      }
    }

    const endTest = (success, error = null) => {
      if (!responseStream.destroyed && !responseStream.writableEnded) {
        try {
          responseStream.write(`data: ${JSON.stringify({ type: 'test_complete', success, error: error || undefined })}\n\n`)
          responseStream.end()
        } catch { /* ignore */ }
      }
    }

    try {
      const fullAccount = await openaiResponsesAccountService.getAccount(accountId)
      if (!fullAccount) {
        throw new Error('Account not found')
      }

      logger.info(`üß™ Testing OpenAI-Responses account connection: ${fullAccount.name} (${accountId})`)

      // Set SSE response headers
      if (!responseStream.headersSent) {
        responseStream.writeHead(200, {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
          'X-Accel-Buffering': 'no'
        })
      }

      sendSSE('test_start', { message: 'Test started' })

      // Build OpenAI-compatible test payload
      const testPayload = {
        model: fullAccount.defaultModel || 'gpt-4o-mini',
        messages: [
          { role: 'system', content: 'You are a helpful assistant. Reply briefly.' },
          { role: 'user', content: 'hi' }
        ],
        max_tokens: 100,
        stream: true
      }

      // Build target URL - use /v1/chat/completions
      const baseUrl = fullAccount.baseApi.replace(/\/$/, '')
      const apiUrl = `${baseUrl}/v1/chat/completions`

      // Configure request
      const requestConfig = {
        method: 'POST',
        url: apiUrl,
        data: testPayload,
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${fullAccount.apiKey}`
        },
        timeout: 30000,
        responseType: 'stream',
        validateStatus: () => true
      }

      // Add custom User-Agent if configured
      if (fullAccount.userAgent) {
        requestConfig.headers['User-Agent'] = fullAccount.userAgent
      }

      // Configure proxy if available
      if (fullAccount.proxy) {
        const proxyAgent = ProxyHelper.createProxyAgent(fullAccount.proxy)
        if (proxyAgent) {
          requestConfig.httpAgent = proxyAgent
          requestConfig.httpsAgent = proxyAgent
          requestConfig.proxy = false
        }
      }

      const response = await axios(requestConfig)

      // Handle non-200 response
      if (response.status !== 200) {
        return new Promise((resolve) => {
          const chunks = []
          response.data.on('data', (chunk) => chunks.push(chunk))
          response.data.on('end', () => {
            const errorData = Buffer.concat(chunks).toString()
            let errorMsg = `API Error: ${response.status}`
            try {
              const json = JSON.parse(errorData)
              errorMsg = json.error?.message || json.message || json.error || errorMsg
            } catch {
              if (errorData.length < 200) errorMsg = errorData || errorMsg
            }
            endTest(false, errorMsg)
            resolve()
          })
          response.data.on('error', (err) => {
            endTest(false, err.message)
            resolve()
          })
        })
      }

      // Process successful streaming response (OpenAI SSE format)
      return new Promise((resolve) => {
        let buffer = ''

        response.data.on('data', (chunk) => {
          buffer += chunk.toString()
          const lines = buffer.split('\n')
          buffer = lines.pop() || ''

          for (const line of lines) {
            if (!line.startsWith('data:')) continue
            const jsonStr = line.substring(5).trim()
            if (!jsonStr || jsonStr === '[DONE]') continue

            try {
              const data = JSON.parse(jsonStr)
              // OpenAI format: choices[0].delta.content
              const content = data.choices?.[0]?.delta?.content
              if (content) {
                sendSSE('content', { text: content })
              }
              // Check for finish
              if (data.choices?.[0]?.finish_reason) {
                sendSSE('message_stop')
              }
            } catch { /* ignore parse errors */ }
          }
        })

        response.data.on('end', () => {
          endTest(true)
          resolve()
        })

        response.data.on('error', (err) => {
          endTest(false, err.message)
          resolve()
        })
      })
    } catch (error) {
      logger.error(`‚ùå Test OpenAI-Responses account connection failed:`, error)
      if (!responseStream.headersSent) {
        responseStream.writeHead(200, {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache'
        })
      }
      endTest(false, error.message)
    }
  }
}

module.exports = new OpenAIResponsesRelayService()
